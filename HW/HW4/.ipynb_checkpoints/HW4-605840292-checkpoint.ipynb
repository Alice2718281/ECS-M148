{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27b7f1cc",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce4be497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive Rate is 0.29\n"
     ]
    }
   ],
   "source": [
    "FPR = 10/(10+25)\n",
    "print(\"False Positive Rate is\", round(FPR,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50063bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Negative Rate is 0.2\n"
     ]
    }
   ],
   "source": [
    "FNR = 9/(9+36)\n",
    "print(\"False Negative Rate is\", round(FNR,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3910f6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25+9+46-59"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebccb1bb",
   "metadata": {},
   "source": [
    "## (b)\n",
    "\n",
    "If we increase the probability threshold t in logistic regression, the model becomes **more conservative in making positive predictions**.  As a result, fewer observations are classified as positive, which means that the false positive rate (FPR) **tends to** decreases and the false negative rate (FNR) tends to increases.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fbddbe",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "065de893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the probability that the smoke detector sounds an alarm is approximately 0.11\n"
     ]
    }
   ],
   "source": [
    "Pro_fire = 0.01*0.99\n",
    "Pro_no_fre = 0.99*(1-0.9)\n",
    "total_prob = Pro_fire+Pro_no_fre\n",
    "print(\"the probability that the smoke detector sounds an alarm is approximately\", round(total_prob,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1206623",
   "metadata": {},
   "source": [
    "## (b)\n",
    "\n",
    "A: Fire occurrence\n",
    "\n",
    "B: Alarm sounds\n",
    "\n",
    "P(B|A) is the probability that the alarm sounds given that there is a fire, which is 0.99. \n",
    "\n",
    "P(A) is the probability of a fire occurring, which is 0.01.\n",
    "\n",
    "P(B) is the probability that the alarm sounds, which is 0.11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03bd9c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given that you heard the alarm sound, the probability that there was actually a fire is 0.09\n"
     ]
    }
   ],
   "source": [
    "P_B_A = 0.99\n",
    "P_A = 0.01\n",
    "P_B = total_prob\n",
    "P_A_B = (P_B_A*P_A)/P_B\n",
    "print(\"Given that you heard the alarm sound, the probability that there was actually a fire is\", round(P_A_B,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad24e42",
   "metadata": {},
   "source": [
    "## (c)\n",
    "\n",
    "Detection of hazardous fire: The smoke detector has a high accuracy rate of 99% in detecting hazardous fires. This indicates that it is reliable in alerting you to potential fire incidents, which is crucial for the safety of the kitchen and the restaurant.\n",
    "\n",
    "False alarms due to cooking smoke: The smoke detector has a 10% false alarm rate when there is no hazardous fire but smoke from cooking is detected.  \n",
    "\n",
    "While false alarms can be inconvenient and disruptive, it is common for smoke detectors to trigger false alarms in such situations. The provided accuracy rate of 90% under non-fire conditions suggests that the detector performs relatively well in distinguishing between actual fires and cooking smoke. Considering these factors, the smoke detector appears to be quite useful overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4423bc",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933dd734",
   "metadata": {},
   "source": [
    "We could set $$\\frac{1}{1+e^{-\\beta^{T}x_i}}=\\sigma(\\beta^{T}x)$$\n",
    "\n",
    "To start, the probability of one data point is: \n",
    "$$P(Y=y|X=x)=\\sigma(\\beta^{T}x)^{y}*[1-\\sigma(\\beta^{T}x)]^{(1-y)}$$\n",
    "\n",
    "Since each datapoint is independent, the probability of all data is: \n",
    "$$L(\\beta)= \\prod_{i=1}^{n} P(Y=y^{(i)}|X=x^{(i)})$$\n",
    "$$L(\\beta)= \\prod_{i=1}^{n} \\sigma(\\beta^{T}x^{(i)})^{y^{(i)}}*[1-\\sigma(\\beta^{T}x^{(i)})]^{(1-y^{(i)})}$$\n",
    "\n",
    "Before we start take derivative, here is the side note of chain rule: \n",
    "$$\\frac{\\partial \\sigma(z)}{\\partial z}=\\sigma(z)[1-\\sigma(z)]$$\n",
    "\n",
    "Derivative of gradient for one datapoint(x,y):\n",
    "$$\\frac{\\partial L(\\beta)}{\\partial \\beta_j}=\\frac{\\partial}{\\partial \\beta_j}ylog\\sigma(\\beta^{T}x)+\\frac{\\partial}{\\partial \\beta_j}(1-y)log[1-\\sigma(\\beta^{T}x)]$$\n",
    "$$=[\\frac{y}{\\sigma(\\beta^{T}x)}-\\frac{1-y}{1-\\sigma(\\beta^{T}x)}]\\frac{\\partial}{\\partial \\beta_j}\\sigma(\\beta^{T}x)$$\n",
    "$$=[\\frac{y}{\\sigma(\\beta^{T}x)}-\\frac{1-y}{1-\\sigma(\\beta^{T}x)}]\\sigma(\\beta^{T}x)[1-\\sigma(\\beta^{T}x)]x_j$$\n",
    "$$=\\frac{y-\\sigma(\\beta^{T}x)}{\\sigma(\\beta^{T}x)[1-\\sigma(\\beta^{T}x)]}\\sigma(\\beta^{T}x)[1-\\sigma(\\beta^{T}x)]x_j$$\n",
    "$$=[y-\\sigma(\\beta^{T}x)]x_j$$\n",
    "Because the derivative of sums is the sum of derivatives, the gradient of theta is simply the sum of this term\n",
    "for each training datapoint:\n",
    "$$\\frac{\\partial L(\\beta)}{\\partial \\beta_j}=\\sum_{i=1}^{n} (\\sigma(\\beta^{T}x)-y_i)x_i^{\\smash{j}}$$\n",
    "$$=\\sum_{i=1}^{n} (\\frac{1}{1+e^{-\\beta^{T}x_i}}-y_i)x_i^{\\smash{j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6100a5",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "## (a) One vs All\n",
    "\n",
    "In the One vs All approach for multi-class classification, we train multiple binary classifiers, where each classifier is designed to distinguish one class from all the other classes.  For example, if we have N classes, we would train N classifiers.  During prediction, we apply each classifier to the input data, and the class with the highest probability or confidence score from the individual classifiers is assigned as the final predicted class.\n",
    "\n",
    "### Advantages of One vs All:\n",
    "\n",
    "Simplicity: The One vs All approach is conceptually straightforward and easy to implement.\n",
    "\n",
    "### Disadvantages of One vs All: \n",
    "\n",
    "Class Overlap: The One vs All approach assumes that the classes are mutually exclusive and independent. However, in some cases, classes may overlap or exhibit complex relationships, which can lead to suboptimal results. And in some cases, no class chose this point. \n",
    "\n",
    "## (b) All vs All\n",
    "\n",
    "In the All vs All approach for multi-class classification, we train binary classifiers for every possible pair of classes.   For N classes, we need ${N \\choose 2}$ classifiers.   During prediction, each classifier votes for its predicted class, and the class with the most votes is assigned as the final predicted class.\n",
    "\n",
    "### Advantages of All vs All:\n",
    "\n",
    "Handling Class Overlap: The All vs All approach can handle situations where classes overlap or have complex relationships since each classifier is trained specifically for a pair of classes.\n",
    "\n",
    "Balanced Training Data: With pairwise classifiers, each classifier is trained on a balanced subset of the data, which can help in achieving better performance.\n",
    "\n",
    "### Disadvantages of All vs All:\n",
    "\n",
    "Complexity: The All vs All approach requires training a large number of classifiers, which can be computationally expensive, especially for a large number of classes.\n",
    "\n",
    "Decision Boundary Ambiguity: In cases where classes have overlapping regions, the decision boundaries learned by different classifiers may conflict or create ambiguous regions, leading to potential misclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aeac69",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "\n",
    "## (a)\n",
    "\n",
    "True:\n",
    "\n",
    "Positive predictive value (PPV), precision, is the probability that a sample is truly positive given that the model classifies it as positive. It is calculated as the number of true positive predictions divided by the sum of true positive and false positive predictions:$PPV = \\frac{\\text{True Positives}} {\\text{True Positives + False Positives}}$\n",
    "\n",
    "## (b)\n",
    "\n",
    "False: \n",
    "\n",
    "total should be **2(k-1)** parameters which is 6\n",
    "\n",
    "## (c)\n",
    "\n",
    "False:\n",
    "\n",
    "Logistic regression models the log-odds as a linear function of the input features. This linear relationship between the log-odds and the input features results in a linear decision boundary in the feature space. The decision boundary is a hyperplane that separates the classes.\n",
    "\n",
    "If the log-odds function is modeled as a quadratic function, it would deviate from the linear relationship and would not represent logistic regression. Instead, it would be a form of non-linear regression.\n",
    "\n",
    "To achieve a non-linear decision boundary in logistic regression, one common approach is to introduce non-linear transformations of the input features. This can be done by adding polynomial terms, interaction terms, or applying other non-linear transformations to the features. By incorporating these non-linear transformations, logistic regression can capture more complex relationships and produce a non-linear decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c06469c",
   "metadata": {},
   "source": [
    "## (d)\n",
    "\n",
    "False:\n",
    "\n",
    "While a 97% test accuracy may initially seem like the model is performing well, it does not provide a comprehensive evaluation of its effectiveness for detecting fraudulent credit card transactions. Accuracy alone is not sufficient for assessing the performance of a classifier, especially in scenarios with imbalanced classes such as fraudulent transactions.\n",
    "\n",
    "In this case, detecting fraudulent transactions is typically a critical task, and false negatives (fraudulent transactions classified as non-fraudulent) can have significant consequences. Simply achieving a high overall accuracy does not necessarily imply that the model is performing well in identifying fraudulent transactions.\n",
    "\n",
    "## (e)\n",
    "\n",
    "True:\n",
    "\n",
    "For a very good classification model, we expect the confusion matrix to be dominated by diagonal entries.  The diagonal entries of the confusion matrix represent the correctly classified instances, where the predicted class matches the true class.  These are the true positives and true negatives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

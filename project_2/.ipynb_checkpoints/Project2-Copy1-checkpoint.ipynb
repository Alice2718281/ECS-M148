{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - Binary Classification Comparative Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we're going to attempt a binary classification of a dataset using multiple methods and compare results. \n",
    "\n",
    "Our goals for this project will be to introduce you to several of the most common classification techniques, how to perform them and tweek parameters to optimize outcomes, how to produce and interpret results, and compare performance. You will be asked to analyze your findings and provide explanations for observed performance. \n",
    "\n",
    "\n",
    "<b><u>DEFINITIONS</b></u>\n",
    "\n",
    "\n",
    "<b> Binary Classification:</b>\n",
    "In this case a complex dataset has an added 'target' label with one of two options. Your learning algorithm will try to assign one of these labels to the data.\n",
    "\n",
    "<b> Supervised Learning:</b>\n",
    "This data is fully supervised, which means it's been fully labeled and we can trust the veracity of the labeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Details\n",
    "\n",
    "**Project is due May 17th at 12:00 pm (Wednesday Noon). To submit the project, please save the notebook\n",
    "as a pdf file and submit the assignment via Gradescope. In addition, make sure that\n",
    "all figures are legible and suﬀiciently large. For best pdf results, we recommend downloading [Latex](https://www.latex-project.org/) and print the notebook using Latex.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Essentials and Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here are a set of libraries we imported to complete this assignment. \n",
    "#Feel free to use these or equivalent libraries for your implementation\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt # this is used for the plot the graph \n",
    "import matplotlib\n",
    "import os\n",
    "import time\n",
    "#Sklearn classes\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC  #SVM classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.metrics.cluster as smc\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, Normalizer, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#Sets random seed\n",
    "import random \n",
    "random.seed(42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function allowing you to export a graph\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function that allows you to draw nicely formatted confusion matrices\n",
    "def draw_confusion_matrix(y, yhat, classes):\n",
    "    '''\n",
    "        Draws a confusion matrix for the given target and predictions\n",
    "        Adapted from scikit-learn and discussion example.\n",
    "    '''\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    matrix = confusion_matrix(y, yhat)\n",
    "    plt.imshow(matrix, interpolation='nearest', cmap=plt.cm.YlOrBr)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    num_classes = len(classes)\n",
    "    plt.xticks(np.arange(num_classes), classes, rotation=90)\n",
    "    plt.yticks(np.arange(num_classes), classes)\n",
    "    \n",
    "    fmt = 'd'\n",
    "    thresh = matrix.max() / 2.\n",
    "    for i, j in itertools.product(range(matrix.shape[0]), range(matrix.shape[1])):\n",
    "        plt.text(j, i, format(matrix[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(data, row_labels, col_labels, figsize = (20,12), cmap = \"YlGn\",\n",
    "            cbar_kw={}, cbarlabel=\"\", valfmt=\"{x:.2f}\",\n",
    "            textcolors=(\"black\", \"white\"), threshold=None):\n",
    "    \"\"\"\n",
    "    Create a heatmap from a numpy array and two lists of labels. \n",
    "    \n",
    "    Taken from matplotlib example.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data\n",
    "        A 2D numpy array of shape (M, N).\n",
    "    row_labels\n",
    "        A list or array of length M with the labels for the rows.\n",
    "    col_labels\n",
    "        A list or array of length N with the labels for the columns.\n",
    "    ax\n",
    "        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n",
    "        not provided, use current axes or create a new one.  Optional.\n",
    "    cmap\n",
    "        A string that specifies the colormap to use. Look at matplotlib docs for information.\n",
    "        Optional.\n",
    "    cbar_kw\n",
    "        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n",
    "    cbarlabel\n",
    "        The label for the colorbar.  Optional.\n",
    "    valfmt\n",
    "        The format of the annotations inside the heatmap.  This should either\n",
    "        use the string format method, e.g. \"$ {x:.2f}\", or be a\n",
    "        `matplotlib.ticker.Formatter`.  Optional.\n",
    "    textcolors\n",
    "        A pair of colors.  The first is used for values below a threshold,\n",
    "        the second for those above.  Optional.\n",
    "    threshold\n",
    "        Value in data units according to which the colors from textcolors are\n",
    "        applied.  If None (the default) uses the middle of the colormap as\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize = figsize)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(data,cmap=cmap)\n",
    "\n",
    "    # Create colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n",
    "    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
    "\n",
    "    # Show all ticks and label them with the respective list entries.\n",
    "    ax.set_xticks(np.arange(data.shape[1]), labels=col_labels)\n",
    "    ax.set_yticks(np.arange(data.shape[0]), labels=row_labels)\n",
    "\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    ax.tick_params(top=True, bottom=False,\n",
    "                   labeltop=True, labelbottom=False)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Turn spines off and create white grid.\n",
    "    ax.spines[:].set_visible(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "    \n",
    "    # Normalize the threshold to the images color range.\n",
    "    if threshold is not None:\n",
    "        threshold = im.norm(threshold)\n",
    "    else:\n",
    "        threshold = im.norm(data.max())/2.\n",
    "\n",
    "    # Set default alignment to center, but allow it to be\n",
    "    # overwritten by textkw.\n",
    "    kw = dict(horizontalalignment=\"center\",\n",
    "              verticalalignment=\"center\")\n",
    "\n",
    "    # Get the formatter in case a string is supplied\n",
    "    if isinstance(valfmt, str):\n",
    "        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
    "\n",
    "    # Loop over the data and create a `Text` for each \"pixel\".\n",
    "    # Change the text's color depending on the data.\n",
    "    texts = []\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
    "            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n",
    "            texts.append(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_meshgrid(x, y, h=0.02):\n",
    "    \"\"\"Create a mesh of points to plot in\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on\n",
    "    y: data to base y-axis meshgrid on\n",
    "    h: stepsize for meshgrid, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "def plot_contours(clf, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = plt.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "def draw_contour(x,y,clf, class_labels = [\"Negative\", \"Positive\"]):\n",
    "    \"\"\"\n",
    "    Draws a contour line for the predictor\n",
    "    \n",
    "    Assumption that x has only two features. This functions only plots the first two columns of x.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    X0, X1 = x[:, 0], x[:, 1]\n",
    "    xx0, xx1 = make_meshgrid(X0,X1)\n",
    "    \n",
    "    plt.figure(figsize = (10,6))\n",
    "    plot_contours(clf, xx0, xx1, cmap=\"PiYG\", alpha=0.8)\n",
    "    scatter=plt.scatter(X0, X1, c=y, cmap=\"PiYG\", s=30, edgecolors=\"k\")\n",
    "    plt.legend(handles=scatter.legend_elements()[0], labels=class_labels)\n",
    "\n",
    "    plt.xlim(xx0.min(), xx0.max())\n",
    "    plt.ylim(xx1.min(), xx1.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Project\n",
    "\n",
    "In this part, we will go over how to perform a Binary classification task using a variety of models. We will provide examples of how to train and evaluate these models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "Healthcare is an important industry that uses machine learning to aid doctors in diagnosing many different kinds of illnesses and diseases. For this example project, we will be using the [Breast Cancer Wisconsin Dataset](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data) to determine whether a mass found in a body is benign or malignant. \n",
    "\n",
    "Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. \n",
    "\n",
    "Feature Information:\n",
    "\n",
    "Column 1: ID number\n",
    "\n",
    "Column 2: Diagnosis (M = malignant, B = benign)\n",
    "\n",
    "Ten real-valued features are computed for each cell nucleus:\n",
    "\n",
    "    1. radius (mean of distances from center to points on the perimeter)\n",
    "    2. texture (standard deviation of gray-scale values)\n",
    "    3. perimeter\n",
    "    4. area\n",
    "    5. smoothness (local variation in radius lengths)\n",
    "    6. compactness (perimeter^2 / area - 1.0)\n",
    "    7. concavity (severity of concave portions of the contour)\n",
    "    8. concave points (number of concave portions of the contour)\n",
    "    9. symmetry\n",
    "    10. fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "Due to the statistical nature of the test, we are not able to get exact measurements of the previous values. Instead, the dataset contains the mean and standard error of the real-valued features. \n",
    "\n",
    "Columns 3-12 present the mean of the measured values\n",
    "\n",
    "Columns 13-22 present the standard error of the measured values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Analyze the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "data = pd.read_csv('datasets/breast_cancer_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always look at your dataset after loading it. Use information from .describe and .info to learn more about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While .info shows that every entry has 569 non-null and there are 569 entries, it is good to explicitly check for nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! No need for imputation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we are looking at the dataset, we shall remove the \"id\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop([\"id\"],axis= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the target labels \n",
    "\n",
    "For this project, we wish to classify the diagnosis column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"diagnosis\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to transform this column into numerical column so that we may use them in our models. To do this, we will employ the LabelEncoder to automatically transform all the target label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder \n",
    "le = LabelEncoder() \n",
    "\n",
    "data['diagnosis'] = le.fit_transform(data['diagnosis'])\n",
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['diagnosis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at a histogram of the full dataset.\n",
    "\n",
    "Its always good to get a global view of your datasets by looking at their histograms. You might see some interesting trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(figsize = (20,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histograms, we can see some interesting trends. Possible observations:\n",
    "\n",
    "- Many of the _se columns indicate a heavy skewness towards low values and have large tails\n",
    "- Many of the _mean columns look more Gaussian in shape\n",
    "- There is a large disparity between the ranges of certain features. For example, radius mean can go from 0 to 25 while smoothness_mean is in the range [0.050,0.150]. This indicates we will have to normalize or standardize the features if the models are sensitive to such measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the correlation matrix to get an idea about which features are important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = data.corr()\n",
    "columns = list(data)\n",
    "#Creates the heatmap\n",
    "heatmap(correlations.values,columns,columns,figsize = (20,12),cmap=\"hsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's specifically look at the correlations of our target feature\n",
    "correlations[\"diagnosis\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a lot of correlation between the features and the target label. Thus, we can expect to learn something from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When doing classification, check if classes are heavily imbalanced.\n",
    "\n",
    "It is important that the dataset does not prefer one class over any others. Otherwise, it may bias the model to not learn the minority classes well. \n",
    "\n",
    "Lets use a histogram and count the number of elements in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['diagnosis'].hist(bins=2, figsize=(5,5))\n",
    "data['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a bit of an imbalance which is something to keep in mind if we find that our models do not perform well on the minority classes. For our purposes, this imbalance is not big enough to be an issue so we will not perform balancing techniques for this dataset.\n",
    "\n",
    "Since the dataset is small though, we want to be careful when making training and testing splits to ensure that there is enough of each class for both splits. We will show how to perform this shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the data\n",
    "\n",
    "Before starting any model training, we have to split up the target labels from our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"diagnosis\"]\n",
    "x = data.drop([\"diagnosis\"],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we also split the data into training and testing data. To ensure that there is not an imbalance of classes in the training and testing set, we will use the stratify parameter in train_test_split to perform stratified sampling on the data (Recall from lecture how stratified sampling is performed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw, test_raw, target, target_test = train_test_split(x,y, test_size=0.2, stratify= y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we named the input feature data as raw to indicate that there has been no pre-processing on them such as standardization. Shortly, we will show the affect that pre-processing has on the performance of the model.\n",
    "\n",
    "Let us quickly test that the splits are somewhat balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training classes\n",
    "target.hist(bins=2, figsize=(5,5))\n",
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing classes\n",
    "target_test.hist(bins=2, figsize=(5,5))\n",
    "target_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the class balance is about the same as before the split. In fact, we can see that if a classifier just guessed class 0, it would have an accuracy of $100 * \\frac{72}{72+42} = 63.15\\%$. We can consider this the baseline accuracy to compare against."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models for Classification: KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first model, we will use KNN classfication. This is a model we have seen many times throughout the course and it would be interesting to see how well it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple KNN classification with K = 3\n",
    "\n",
    "Let us try KNN on the raw data with simply 3 nearest neighbors. We use the sklearn [metric library](https://scikit-learn.org/stable/modules/model_evaluation.html) to calculate the measures of interest. In this case, we focus on accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-Nearest Neighbors algorithm\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(train_raw, target)\n",
    "predicted = knn.predict(test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%-12s %f\" % ('Accuracy:', metrics.accuracy_score(target_test,predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is already a huge improvement in accuracy on comparison to the baseline of 63.15%. Let's see the effect that standardizing the input features would have on the KNN performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affect of pre-processing on KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since all features are real-valued, we only have one pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "#Transform raw data \n",
    "train = pipeline.fit_transform(train_raw)\n",
    "test = pipeline.transform(test_raw) #Note that there is no fit calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-Nearest Neighbors algorithm\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(train, target)\n",
    "testing_result = knn.predict(test)\n",
    "predicted = knn.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%-12s %f\" % ('Accuracy:', metrics.accuracy_score(target_test,predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with pre-processing we were able to get a much better classification accuracy. \n",
    "\n",
    "Here we only used StandardScaler. Lets see if other pre-processing techniques could have also worked. As such, lets  look at MinMaxScaler and Normalizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessors = [StandardScaler(),MinMaxScaler(),Normalizer() ]\n",
    "\n",
    "for pre in preprocessors:\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', pre)\n",
    "    ])\n",
    "\n",
    "\n",
    "    #Transform raw data \n",
    "    train = pipeline.fit_transform(train_raw)\n",
    "    test = pipeline.transform(test_raw) #Note that there is no fit calls\n",
    "    # k-Nearest Neighbors algorithm\n",
    "    knn = KNeighborsClassifier(n_neighbors=7)\n",
    "    knn.fit(train, target)\n",
    "    testing_result = knn.predict(test)\n",
    "    predicted = knn.predict(test)\n",
    "    \n",
    "    print(pre)\n",
    "    print(\"%-12s %f\" % ('Accuracy:', metrics.accuracy_score(target_test,predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that MinMaxScaler had the same performance as StandardScaler. Yet, Normalizer did not improve the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing decision boundaries  for KNN\n",
    "\n",
    "Its always nice to see the decision boundaries a model decides upon. Let's see how the decision boundary changes as function of k when only using the two most correlated features to the target labels: concave_points_mean and perimeter_mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract first two features and use the standardscaler \n",
    "train_2 = StandardScaler().fit_transform(train_raw[['concave points_mean','perimeter_mean']])\n",
    "\n",
    "k_r  = [1,3,5,7]\n",
    "for k in k_r:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(train_2, target)\n",
    "    draw_contour(train_2,target,knn, class_labels = ['Benign', 'Malignant'])\n",
    "\n",
    "    plt.title(f\"K ={k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that as k gets larger, the decision boundary gets smoother. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models for Classification: Logistic Regression\n",
    "\n",
    "While KNN is a very powerful model, it does come with a few issues such as \n",
    "\n",
    "- Require storing the full training dataset\n",
    "- Prediction is done by comparing new sample will all samples in training set which is time-consuming\n",
    "\n",
    "These issues arise because KNN is a **non-parametric** model which means that it does not summarize the data into a finite set of parameters.\n",
    "\n",
    "Let us now look at Logistic Regression which is an example of a **parametric** model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us see how logistic regression performs without any regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(penalty = \"l2\",max_iter = 1000, solver = \"lbfgs\", C=(10**30)/1) \n",
    "#C is choosen to be high to remove regularization\n",
    "#We could have chosen penalty = \"none\" since lbfgs supports it but this option is not possible for all solvers.\n",
    "\n",
    "log_reg.fit(train_raw, target)\n",
    "testing_result = log_reg.predict(test_raw)\n",
    "predicted = log_reg.predict(test_raw)\n",
    "\n",
    "print(\"%-12s %f\" % ('Accuracy:', metrics.accuracy_score(target_test,predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Logistic Regression is actually performing much better than any of the KNN models we tried. We can also see the parameters that the model learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for each feature\n",
    "log_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intercept term\n",
    "log_reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Features in data:\", train_raw.shape[1])\n",
    "print(\"Number of Parameters:\", len(log_reg.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using Logistic Regression where we model the log odds with a linear function, it makes sense that we have a parameter/coefficient for each input feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Parameters for Logistic Regression\n",
    "\n",
    "In Sci-kit Learn, the following are just some of the parameters we can pass into Logistic Regression:\n",
    "\n",
    "- penalty: {‘l1’, ‘l2’, ‘elasticnet’, ‘none’} default=\"l2\"\n",
    "    - Specifies the type of regularization to use. Not all penalties work for each solver. \n",
    "- C: positive float, default=1\n",
    "    - Inverse of the regularization strength. You can treat C as $\\frac{1}{\\lambda}$ as shown in lecture. Thus, as C gets smaller, the regularization strength increases.\n",
    "- solver: {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’\n",
    "    - Algorithm to use in the optimization problem. Each algorithms solves logistic regression using different iterative methods that are based on the gradient. Read the [sci-kit learn documentation](https://scikit-learn.org/dev/modules/linear_model.html#logistic-regression) for more information.\n",
    "- max_iter: int, default=100\n",
    "    - Maximum number of iterations taken for the solvers to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each parameter has a different effect on the model. Let's look at how the choose of max_iter affects the model performance on the raw data and the standardized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Since all features are real-valued, we only have one pipeline\n",
    "preprocesser = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "#Transform raw data \n",
    "train = preprocesser.fit_transform(train_raw)\n",
    "test = preprocesser.transform(test_raw) #Note that there is no fit call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(penalty = \"l2\",max_iter = 1000, solver = \"lbfgs\", C= 0.01) \n",
    "\n",
    "#Train raw is the data before preprocessing\n",
    "log_reg.fit(train_raw, target)\n",
    "predicted = log_reg.predict(test_raw)\n",
    "print(\"%-12s %f\" % ('Raw Data Accuracy:', metrics.accuracy_score(target_test,predicted)))\n",
    "\n",
    "\n",
    "#Train is the data after preprocessing (using Standard scalar)\n",
    "log_reg.fit(train, target)\n",
    "predicted = log_reg.predict(test)\n",
    "\n",
    "print(\"%-12s %f\" % ('Preprocessed Data Accuracy:', metrics.accuracy_score(target_test,predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the accuraccies are pretty close to each other.  Lets see what happens when we decrease the max_iter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(penalty = \"l2\",max_iter = 70, solver = \"lbfgs\", C= 0.01) \n",
    "\n",
    "#Train raw is the data before preprocessing\n",
    "log_reg.fit(train_raw, target)\n",
    "predicted = log_reg.predict(test_raw)\n",
    "print(\"%-12s %f\" % ('Raw Data Accuracy:', metrics.accuracy_score(target_test,predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ooops! The model did not seem to converge. Its seem that the scale of the features strongly affects the convergence speed of the iterative algorithm. As suggested, we can fix this issue by increaing the max_iter, re-scaling the data, or using a different solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Train is the data after preprocessing (using Standard scalar)\n",
    "log_reg.fit(train, target)\n",
    "predicted = log_reg.predict(test)\n",
    "\n",
    "print(\"%-12s %f\" % ('Preprocessed Data Accuracy:', metrics.accuracy_score(target_test,predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do a little experiment using cross validation to see how each term affects the logistic regression. We will perform this example on the standardized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#You may even do Cross validation for classification\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Note that this a list of dict\n",
    "#Each dict describes the combination of parameters to check\n",
    "parameters = [\n",
    "    {\"penalty\": [\"l2\"],\n",
    "    \"C\": [0.01,1,100],\n",
    "    \"solver\": [\"lbfgs\",\"liblinear\"]},#These solvers support penalty = \"l2\"\n",
    "    {\"penalty\": [\"none\"],\n",
    "    \"C\": [1], #Specified to prevent error message\n",
    "    \"solver\": [\"lbfgs\",\"newton-cg\"]},#These solvers support penalty = \"none\"\n",
    "]\n",
    "\n",
    "#instantiate model\n",
    "\n",
    "\n",
    "#Implementing cross validation\n",
    "\n",
    "k = 3\n",
    "kf = KFold(n_splits=k, random_state=None)\n",
    "\n",
    "log_reg = LogisticRegression(penalty = \"none\",max_iter = 1000, solver = \"lbfgs\") #will change parameters during CV\n",
    "grid = GridSearchCV(log_reg , parameters, cv = kf, scoring = \"accuracy\")\n",
    "grid.fit(train,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put results into Dataframe\n",
    "res= pd.DataFrame(grid.cv_results_)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the columns that specify the score and the parameters for each row\n",
    "res[[\"rank_test_score\",\"param_C\",\"param_penalty\",\"param_solver\",\"mean_test_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the choice of these parameters can stronlgy affect performance of the classifier. Lets check the performance of the best parameters on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Train raw is the data before preprocessing\n",
    "predicted = grid.predict(test)\n",
    "print(\"%-12s %f\" % ('Accuracy:', metrics.accuracy_score(target_test,predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this test accuracy is not as good as some of the other logistic regression examples we've shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speedtest between KNN and Logistic Regression\n",
    "\n",
    "Lets see how long KNN and Logistic Regression take to perform training and testing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train = scaler.fit_transform(train_raw)\n",
    "test = scaler.fit_transform(test_raw)\n",
    "\n",
    "\n",
    "log_reg = LogisticRegression(penalty = \"none\",max_iter = 1000)\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "t0 = time.time()\n",
    "knn.fit(train, target)\n",
    "t1 = time.time()\n",
    "print(\"KNN Training Time : \", t1-t0)\n",
    "\n",
    "t0 = time.time()\n",
    "log_reg.fit(train, target)\n",
    "t1 = time.time()\n",
    "print(\"Logistic Regression Training Time : \", t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "knn.predict(test)\n",
    "t1 = time.time()\n",
    "print(\"KNN Testing Time : \", t1-t0)\n",
    "\n",
    "t0 = time.time()\n",
    "log_reg.predict(test)\n",
    "t1 = time.time()\n",
    "print(\"Logistic Regression Testing Time : \", t1-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple test shows that Logistic Regression is slower than KNN during Training time but is much faster during testing time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing decision boundaries for Logistic Regression\n",
    "\n",
    "Now, lets look at the decision boundary caused by Logistic Regression. Same as for KNN, we use the two most correlated features to the target labels: concave_points_mean and perimeter_mean. This way, we can visualize the 2D decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract first two feature and use the standardscaler \n",
    "train_2 = StandardScaler().fit_transform(train_raw[['concave points_mean','perimeter_mean']])\n",
    "\n",
    "Cs  = [0.001,0.1,1000]\n",
    "for C in Cs:\n",
    "    log_reg = LogisticRegression(penalty = \"l2\",max_iter = 1000, solver = \"lbfgs\", C=C) #will change parameters during CV\n",
    "    log_reg.fit(train_2, target)\n",
    "    \n",
    "    draw_contour(train_2,target,log_reg, class_labels = ['Benign', 'Malignant'])\n",
    "    plt.title(f\"C ={C}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see as the regularization strength changes, the decision boundary moves as well. Additionally, we can clearly see that the decision boundary is a line since this is a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models for Classification: SVM\n",
    "\n",
    "We now discuss another type of linear classification model known as Support Vector Machines (SVM). Where Logistic Regression was motivated probability theory, SVM is motivated by geometeric arguments. Specifcally, SVM finds a separating hyperline that maximizes the margin (i.e. distance from each class). The hyperplane is used to classify the points by designating every sample on of side of the hyperplane as the positive class and the other side as the negative class.\n",
    "\n",
    "The hyperplane is determine by a few sample points known as support vectors that uniquely characterize the hyperplane.\n",
    "\n",
    "\n",
    "![svm_im](images/support-vector-machine-algorithm.png)\n",
    "\n",
    "\n",
    "Note that it may not always be possible to find a hyperplane that completely separates the classes. Thus, we use what is known as Soft-Margin SVM which aims to maximize the margin while minizming the distance on the classes that are on the wrong side.\n",
    "\n",
    "All Sci-kit learn implementations of SVM that we use are soft-margin SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple SVM classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "svm.fit(train, target)\n",
    "predicted = svm.predict(test)\n",
    "print(\"%-12s %f\" % ('Accuracy:', metrics.accuracy_score(target_test,predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Parameters for SVM\n",
    "\n",
    "In Sci-kit Learn, the following are just some of the parameters we can pass into Logistic Regression:\n",
    "\n",
    "- C: positive float, default=1\n",
    "    - Inverse of the regularization strength. You can treat C as $\\frac{1}{\\lambda}$ as shown in lecture. Thus, as C gets smaller, the regularization strength increases. SVM only uses the L2 regularization.\n",
    "- kernel: {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’}, default=’rbf’\n",
    "    - Specifies the kernel type to be used in the algorithm. A kernel specifies a mapping into a higher dimension space to allow for non-linear decision boundaries.\n",
    "- degree: int, default=3\n",
    "    - Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing decision boundaries for SVM\n",
    "\n",
    "Now, lets look at the decision boundary caused by SVM with different kernels. Same as for KNN and Logistic Regression, we use the two most correlated features to the target labels: concave_points_mean and perimeter_mean. This way, we can visualize the 2D decision boundary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract first two feature and use the standardscaler \n",
    "train_2 = StandardScaler().fit_transform(train_raw[['concave points_mean','perimeter_mean']])\n",
    "\n",
    "kernel  = ['linear', 'poly', 'rbf',  'sigmoid']\n",
    "for ker in kernel:\n",
    "    svm = SVC(kernel = ker) #will change parameters during CV\n",
    "    svm.fit(train_2, target)\n",
    "    draw_contour(train_2,target,svm,class_labels = ['Benign', 'Malignant'])\n",
    "    \n",
    "    plt.title(f\"Kernel ={ker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the decision boundary is not always linear because we are using non-linear kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Measures for Classifications\n",
    "\n",
    "Now that we have gone over a few models for binary classification, let's explore the different ways we can measure the performance of these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example classifier\n",
    "log_reg = LogisticRegression(max_iter = 1000)\n",
    "log_reg.fit(train_raw, target)\n",
    "predicted = log_reg.predict(test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are just some of the most important measures of interest. We use the convention to refer to the class labeled as $1$ as the positive class.\n",
    "\n",
    "- **Accuracy:** The percentage of predictions that are correct. Use metrics.accuracy_score\n",
    "- **Precision:** $\\frac{\\text{Number of labels correctly classified as positive}}{\\text{Number of labels classified as positives}}$. Percentage of predictions that are correctly positive among all the predictions that were classified as positive. Use metrics.precision_score\n",
    "- **Recall:** $\\frac{\\text{Number of labels correctly classified as positive}}{\\text{Number of labels where the true class is positive}}$. Percentage of predictions that are correctly positive among all the labels where the true class is positive. Also known as the probability of detecting when a class is positive. Use metrics.recall_score\n",
    "- **F1 Score:** Harmonic mean of the precision and recall. Highest value is $1$ when both precision and recall are $1$, i.e. perfect. Lowest value is $0$ when either precision or recall is zero. Provides an aggregate score to analyze both precision and recall. Use metrics.f1_score\n",
    "\n",
    "We can calculate these measures by using a confusion matrix as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%-12s %f\" % ('Accuracy:', metrics.accuracy_score(target_test,predicted)))\n",
    "print(\"%-12s %f\" % ('Precision:', metrics.precision_score(target_test,predicted, labels=None, pos_label=1, average='binary', sample_weight=None)))\n",
    "print(\"%-12s %f\" % ('Recall:', metrics.recall_score(target_test,predicted, labels=None, pos_label=1, average='binary', sample_weight=None)))\n",
    "print(\"%-12s %f\" % ('F1 Score:', metrics.f1_score(target_test,predicted, labels=None, pos_label=1, average='binary', sample_weight=None)))\n",
    "print(\"Confusion Matrix: \\n\", metrics.confusion_matrix(target_test,predicted))\n",
    "\n",
    "#Draws confusion matrix\n",
    "draw_confusion_matrix(target_test, predicted, ['Benign', 'Malignant'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Using classification methods to classify heart disease\n",
    "\n",
    "Now that you have some examples of the classifiers that Sci-kit learn has to offers, let try to apply them to a new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: The Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we will be using a subset of the UCI Heart Disease dataset, leveraging the fourteen most commonly used attributes. All identifying information about the patient has been scrubbed. You will be asked to classify whether a <b>patient is suffering from heart disease</b> based on a host of potential medical factors.\n",
    "\n",
    "The dataset includes 14 columns. The information provided by each column is as follows:\n",
    "<ul>\n",
    "    <li><b>age:</b> Age in years</li>\n",
    "    <li><b>sex:</b> (1 = male; 0 = female)</li>\n",
    "    <li><b>cp:</b> Chest pain type (0 = asymptomatic; 1 = atypical angina; 2 = non-anginal pain; 3 = typical angina)</li>\n",
    "    <li><b>trestbps:</b> Resting blood pressure (in mm Hg on admission to the hospital)</li>\n",
    "    <li><b>chol:</b> cholesterol in mg/dl</li>\n",
    "    <li><b>fbs</b> Fasting blood sugar > 120 mg/dl (1 = true; 0 = false)</li>\n",
    "    <li><b>restecg:</b> Resting electrocardiographic results (0= showing probable or definite left ventricular hypertrophy by Estes' criteria; 1 = normal; 2 = having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV))</li>\n",
    "    <li><b>thalach:</b> Maximum heart rate achieved</li>\n",
    "    <li><b>exang:</b> Exercise induced angina (1 = yes; 0 = no)</li>\n",
    "    <li><b>oldpeak:</b> Depression induced by exercise relative to rest</li>\n",
    "    <li><b>slope:</b> The slope of the peak exercise ST segment (0 = downsloping; 1 = flat; 2 = upsloping)</li>\n",
    "    <li><b>ca:</b> Number of major vessels (0-3) colored by flourosopy</li>\n",
    "    <li><b>thal:</b> 1 = normal; 2 = fixed defect; 7 = reversable defect</li>\n",
    "    <li><b><u>sick:</u></b> Indicates the presence of Heart disease (True = Disease; False = No disease)</li>\n",
    "</ul>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [25 pts] Part 1. Load the Data and Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load our dataset so we'll be able to work with it. (correct the relative path if your notebook is in a different directory than the csv file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('datasets/heartdisease.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 pts] Looking at the data \n",
    "\n",
    "Now that our data is loaded, let's take a closer look at the dataset we're working with. Use the head method,  the describe method, and the info method to display some of the rows so we can visualize the types of data fields we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sometimes data will be stored in different formats (e.g., string, date, boolean), but many learning methods work strictly on numeric inputs. Additionally, some numerical features can represent categorical features which need to be pre-processed.  Are there any columns that need to be transformed and why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Use this area to describe any fields you believe will be problemmatic and why] E.g., All the columns in our dataframe are numeric (either int or float), however our target variable 'sick' is a boolean and may need to be modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determine if we're dealing with any null values. If so, report on which columns?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Discuss here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 pts] Transform target label into numerical value\n",
    "\n",
    "Before we begin our analysis, we need to fix the field(s) that will be problematic. Specifically, convert our boolean \"sick\" variable into a binary numeric target variable (values of either '0' or '1') using the [label encoder from scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html), place this new array into a new column of the DataFrame named \"target\", and then drop the original \"sick\" column from the dataframe. Afterward, use .head to print the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 pts] Plotting histogram of data\n",
    "\n",
    "Now that we have a feel for the data-types for each of the variables, plot histograms of each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 pts] Looking at class balance\n",
    "\n",
    "We also want to make sure we are dealing with a balanced dataset. In this case, we want to confirm whether or not we have an equitable number of  sick and healthy individuals to ensure that our classifier will have a sufficiently balanced dataset to adequately classify the two. Plot a histogram specifically of the sick target, and conduct a count of the number of sick and healthy individuals and report on the results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Include description of findings here] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced datasets are important to ensure that classifiers train adequately and don't overfit, however arbitrary balancing of a dataset might introduce its own issues. \n",
    "\n",
    "**Discuss some of the problems that might arise by artificially balancing a dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Discuss prompt here] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 pts] Looking at Data Correlation\n",
    "\n",
    "Now that we have our dataframe prepared let's start analyzing our data. For this next question let's look at the correlations of our variables to our target value. First, use the heatmap function to plot the correlations of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, show the correlation to the \"target\" feature only and sorr them in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the heatmap values and the description of the features, why do you think some variables correlate more highly than others?**  (This question is just to get you thinking and there is no perfect answer since we have no medical background)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Discuss correlations here] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [25 pts] Part 2. Prepare the Data and run a KNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running our various learning methods, we need to do some additional prep to finalize our data. Specifically you'll have to cut the classification target from the data that will be used to classify, and then you'll have to divide the dataset into training and testing cohorts.\n",
    "\n",
    "Specifically, we're going to ask you to prepare 2 batches of data. The first batch will simply be the raw numeric data that hasn't gone through any additional pre-processing. The second batch will be data that you will pipeline using pre-processing methods. We will then feed both of these datasets into a classifier to showcase just how important this step can be!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2 pts]  Separate target labels from data\n",
    "\n",
    "Save the label column as a separate array and then make a new dataframe without the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 pts] Balanced Train Test Split\n",
    "\n",
    "Now, create your 'Raw' unprocessed training data by dividing your dataframe into training and testing cohorts, with your training cohort consisting of 60% of your total dataframe. To ensure that the train and test sets have balanced classes, use the [stratify command of train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). Output the resulting shapes of your training and testing samples to confirm that your split was successful. Additionally, output the class counts for the training and testing cohorts to confirm that there is no artifical class imbalance.\n",
    "\n",
    "Note: Use randomstate = 0 to ensure that the same train/test split happens everytime for ease of grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 pts] KNN on raw data \n",
    "\n",
    "Now, let's try a classification model on this data. We'll first use KNN since it is the one we are most familiar with. \n",
    "\n",
    "One thing we noted in class was that because KNN relies on Euclidean distance, it is highly sensitive to the relative magnitude of different features. Let's see that in action! Implement a K-Nearest Neighbor algorithm on our data and report the results. For this initial implementation, simply use the default settings. Refer to the [KNN Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) for details on implementation. **Report on the test accuracy of the resulting model and print out the confusion matrix.**\n",
    "\n",
    "Recall that accurracy can be calculated easily using metrics.accuracy_score and that we have a helper function to draw the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 pts] KNN on preprocessed data\n",
    "Now lets implement a pipeline to preprocess the data. For the pipeline, use StandardScaler on the numerical features and one-hot encoding on the categorical features. For reference on how to make a pipeline, please look at project 1.\n",
    "\n",
    "For reference, the categorical features are ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca','thal']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now use the pipeline to transform the data and then apply the same KNN classifier with this new training/testing data. Report the test accuraccy. Discuss the implications of the different results you are obtaining.**\n",
    "\n",
    "Note: Remember to use fit_transform on the training data and transform on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Discuss Results here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [8 pts] KNN Parameter optimization for n_neighbors\n",
    "\n",
    "The KNN Algorithm includes an n_neighbors attribute that specifies how many neighbors to use when developing the cluster. (The default value is 5, which is what your previous model used.) Lets now try n values of: 1, 2, 3, 5, 7, 9, 10, 20, and 50. Run your model for each value and report the test accuracy for each. (HINT leverage python's ability to loop to run through the array and generate results without needing to manually code each iteration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment for which value of n did the KNN model perform the best.  Did the model perform strictly better or stricly worse as the value of n increased?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Answer Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a model that seems to work well. But let's see if we can do better! To do so we'll employ Logistic Regression and SVM to improve upon the model and compare the results.\n",
    "\n",
    "**For the rest of the project, you will only be using the transformed data and not the raw data. DO NOT USE THE RAW DATA ANYMORE.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 pts] Part 3. Additional Learning Methods:  Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try Logistic Regression. Recall that Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 pts] Run the default Logistic Regression\n",
    "\n",
    "Implement a Logistical Regression Classifier. Review the [Logistical Regression Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for how to implement the model.  Use the default settings. **Report on the test accuracy and print out the confusion matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 pts] Compare Logistic Regression and  KNN\n",
    "In your own words, describe the key differences between Logistic Regression and KNN? When would you use one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Provide your response here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 pts] Tweaking the Logistic Regression\n",
    "\n",
    "**What are some parameters we can change that will affect the performance of Logistic Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Provide answer to prompt here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Implement Logistic Regression with solver= 'liblinear', max_iter= 1000, penalty = 'l2', and C=1. Report on the test accuracy and print out the confusion matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, Implement Logistic Regression with solver= 'liblinear', max_iter= 1000, penalty = 'l2', and C=0.0001. Report on the test accuracy and print out the confusion matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Did the accuraccy drop or improve? Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Provide answer to prompt here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 pts] Trying out different penalties\n",
    "\n",
    "**Now, Implement Logistic Regression with solver= 'liblinear', max_iter= 1000, penalty = 'l1', and C=1. Report on the test accuracy and print out the confusion matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describe what the purpose of a penalty term is and how the change from L2 to L1 affected the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Discuss prompt here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 pts] Part 4. Additional Learning Methods:  SVM (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples. In two dimensional space this hyperplane is a line dividing a plane in two parts each corresponding to one of the two classes.\n",
    "\n",
    "Recall that Sci-kit learn uses soft-margin SVM to account for datasets that are not separable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 pts] Run default SVM classifier\n",
    "\n",
    "Implement a Support Vector Machine classifier on your pipelined data. Review the [SVM Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) for how to implement a model. For this implementation you can simply use the default settings. **Report on the test accuracy and print out the confusion matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print out the number of support vectors that SVC has determined. Look at the documentation for how to get this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find that there are quite a few support vectors. This is due in part to the small number of samples in the training set and the choice of kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 pts] Use a Linear SVM\n",
    "\n",
    "Rerun your SVM, but now modify your model parameter kernel to equal 'linear'. **Report on the test accuracy and print out the confusion matrix. Also, print out the number of support vectors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that number of support vectors has decreased significantly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 pts] Compare default SVM and Linear SVM\n",
    "\n",
    "Explain the what the new results you've achieved mean. Read the documentation to understand what you've changed about your model and explain why changing that input parameter might impact the results in the manner you've observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Discuss Prompt here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 pts] Compare SVM and Logistic Regression\n",
    "\n",
    "Both logistic regression and linear SVM are trying to classify data points using a linear decision boundary but achieve it in different ways. In your own words, explain the difference between the ways that Logistic Regression and Linear SVM find the boundary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Provide Answer here:] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10 pts] Part 5: Cross Validation and Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've sampled a number of different classification techniques and have seen their performance on the dataset. \n",
    "Before we draw any conclusions on which model is best, we want to ensure that our results are not the result of the random sampling of our data we did with the Train-Test-Split. To ensure otherwise, we will conduct a K-Fold Cross-Validation with GridSearch to determine which model perform best and assess its performance on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 pts]  Model Selection\n",
    "\n",
    "Run a GridSearchCV with 3-Fold Cross Validation. You will be running each classification model with different parameters. \n",
    "\n",
    "KNN:\n",
    "- n_neighbors = [1,3,5,7]\n",
    "- metric = [\"euclidean\",\"manhattan\"] #Different Distance functions\n",
    "\n",
    "Logistic Regression:\n",
    "- penalty = [\"l1\",\"l2\"]\n",
    "- solver = [\"liblinear\"]\n",
    "- C = [0.0001,0.1,10]\n",
    "\n",
    "SVM:\n",
    "- kernel = [\"linear\",\"rbf\"]\n",
    "- C = [0.0001,0.1,10]\n",
    "\n",
    "\n",
    "Make sure to train and test your model on the transformed data and not on the raw data.\n",
    "\n",
    "After using GridSearchCV, put the results into a pandas Dataframe and print out the whole table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What was the best model and what was it's score?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the best model you have, report the test accuracy and print out the confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "398.047px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
